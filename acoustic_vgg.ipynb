{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see acoustic_multichannel for downloading data (migrate to README later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, 'models/research/audioset/vggish/')\n",
    "\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "import vggish_slim\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.11/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/opt/mamba/lib/python3.11/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:318: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/research/audioset/vggish//vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 16:43:06.110628: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle VGGish\n",
    "vgg_graph = tf.Graph()\n",
    "with vgg_graph.as_default():\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "    vggish_slim.define_vggish_slim(training=True)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, 'models/research/audioset/vggish//vggish_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données d'entrée (RIRs) et de sortie (positions reconnues) depuis les fichiers .npy\n",
    "input_data = np.load('data/LivingRoom_preprocessed_hack/Human1/deconvoled_trim.npy')\n",
    "output_data = np.load('data/LivingRoom_preprocessed_hack/Human1/centroid.npy')\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "train_input, test_input, train_output, test_output = train_test_split(input_data, output_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des caractéristiques avec VGGish\n",
    "def extract_vggish_features(data):\n",
    "    features = []\n",
    "    with vgg_graph.as_default():\n",
    "        for rir in data:\n",
    "            # Prétraitement des RIRs\n",
    "            # Ici, vous devriez adapter le prétraitement en fonction de vos données RIRs\n",
    "            # Par exemple, redimensionnement, normalisation, etc.\n",
    "            # Ensuite, convertissez les RIRs en spectrogrammes\n",
    "            spectrogram = vggish_input.waveform_to_examples(rir, vggish_params.SAMPLE_RATE)\n",
    "            # Extraction des caractéristiques avec VGGish\n",
    "            features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n",
    "            embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n",
    "            embedding = sess.run(embedding_tensor, feed_dict={features_tensor: spectrogram})\n",
    "            features.append(embedding)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 4, 667200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des caractéristiques pour les ensembles d'entraînement et de test\n",
    "# train_features = extract_vggish_features(train_input)\n",
    "# test_features = extract_vggish_features(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
